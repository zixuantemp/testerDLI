{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43b2458c",
      "metadata": {
        "id": "43b2458c"
      },
      "source": "#  Phishing-URL-Detection - Group Integration"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4df4efa7",
      "metadata": {
        "id": "4df4efa7"
      },
      "outputs": [],
      "source": "# Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sklearn and friends\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, precision_recall_curve\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Optional SMOTE\ntry:\n    from imblearn.over_sampling import SMOTE\n    SMOTE_AVAILABLE = True\nexcept Exception:\n    SMOTE_AVAILABLE = False\n    try:\n        import sys, subprocess\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"imbalanced-learn\", \"-q\"])\n        from imblearn.over_sampling import SMOTE\n        SMOTE_AVAILABLE = True\n    except Exception:\n        SMOTE_AVAILABLE = False\n\nimport warnings, os, io, requests, pickle, time\nwarnings.filterwarnings(\"ignore\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49473dd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49473dd2",
        "outputId": "614cf5d8-d4c1-42e4-a22a-1b7ce253fb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset shape: (11054, 32)\\nTrain size: (8843, 31) | Test size: (2211, 31)\\n"
        }
      ],
      "source": "# Download dataset directly from GitHub (raw)\nRAW_URL = \"https://raw.githubusercontent.com/Jerrell-Su/DLI_GroupAJ/main/data/phishing.csv\"\n\ndef load_dataset_from_github(url: str) -> pd.DataFrame:\n    try:\n        df = pd.read_csv(url)\n        return df\n    except Exception:\n        import requests, io\n        resp = requests.get(url, timeout=60)\n        resp.raise_for_status()\n        return pd.read_csv(io.StringIO(resp.text))\n\ndataset = load_dataset_from_github(RAW_URL)\nprint(\"Dataset shape:\", dataset.shape)\nX = dataset.drop([\"class\"], axis=1)\ny = dataset[\"class\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(\"Train size:\", X_train.shape, \"| Test size:\", X_test.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6bf6467",
      "metadata": {
        "id": "f6bf6467"
      },
      "outputs": [],
      "source": "def evaluate_model(model, model_name, X_train, X_test, y_train, y_test, training_time=0.0):\n    # Evaluate a model and return metrics as a dict.\n\n    # Predict\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    # Validate labels\n    valid_labels = {-1, 0, 1}\n    if not (set(y_train).issubset(valid_labels) and set(y_test).issubset(valid_labels)):\n        raise ValueError(\"Unexpected labels in targets. Expected subset of {-1,0,1}.\")\n\n    if not (set(y_train_pred).issubset(valid_labels) and set(y_test_pred).issubset(valid_labels)):\n        raise ValueError(\"Unexpected labels in predictions. Expected subset of {-1,0,1}.\")\n\n    # Map targets to [0,1] if needed\n    y_train_mapped = y_train.copy()\n    y_test_mapped = y_test.copy()\n    if set(y_train).issubset({-1, 1}):\n        y_train_mapped = (y_train == 1).astype(int)\n    if set(y_test).issubset({-1, 1}):\n        y_test_mapped = (y_test == 1).astype(int)\n\n    # Map predictions to [0,1] if needed\n    y_train_pred_mapped = y_train_pred.copy()\n    y_test_pred_mapped = y_test_pred.copy()\n    if set(y_train_pred).issubset({-1, 1}):\n        y_train_pred_mapped = (y_train_pred == 1).astype(int)\n    if set(y_test_pred).issubset({-1, 1}):\n        y_test_pred_mapped = (y_test_pred == 1).astype(int)\n\n    # Metrics\n    metrics = {\n        \"Model\": model_name,\n        \"Training_Time\": f\"{training_time:.2f}s\",\n        \"Train_Accuracy\": accuracy_score(y_train_mapped, y_train_pred_mapped),\n        \"Test_Accuracy\": accuracy_score(y_test_mapped, y_test_pred_mapped),\n        \"Train_F1\": 0.0,\n        \"Test_F1\": 0.0,\n        \"Train_Recall\": 0.0,\n        \"Test_Recall\": 0.0,\n        \"Train_Precision\": 0.0,\n        \"Test_Precision\": 0.0,\n    }\n\n    metrics[\"Train_F1\"] = f1_score(y_train_mapped, y_train_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n    metrics[\"Test_F1\"] = f1_score(y_test_mapped, y_test_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n    metrics[\"Train_Recall\"] = recall_score(y_train_mapped, y_train_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n    metrics[\"Test_Recall\"] = recall_score(y_test_mapped, y_test_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n    metrics[\"Train_Precision\"] = precision_score(y_train_mapped, y_train_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n    metrics[\"Test_Precision\"] = precision_score(y_test_mapped, y_test_pred_mapped, average=\"binary\", pos_label=1, zero_division=0)\n\n    return metrics"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9af6c861",
      "metadata": {
        "id": "9af6c861"
      },
      "outputs": [],
      "source": "class UltimateOptimizedModel:\n    def __init__(self, stacking_model, threshold, scaler, smote_model=None):\n        self.stacking_model = stacking_model\n        self.threshold = threshold\n        self.scaler = scaler\n        self.smote_model = smote_model\n\n    def predict(self, X):\n        if hasattr(X, \"iloc\"):\n            X_scaled = self.scaler.transform(X)\n        else:\n            X_scaled = X\n        probas = self.stacking_model.predict_proba(X_scaled)[:, 1]\n        return (probas >= self.threshold).astype(int)\n\n    def predict_proba(self, X):\n        if hasattr(X, \"iloc\"):\n            X_scaled = self.scaler.transform(X)\n        else:\n            X_scaled = X\n        return self.stacking_model.predict_proba(X_scaled)"
    },
    {
      "cell_type": "markdown",
      "id": "2a0b8c8e",
      "metadata": {
        "id": "2a0b8c8e"
      },
      "source": "## Model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa69e726",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "aa69e726",
        "outputId": "4ee2b468-cf47-4707-fd90-a0193018136a"
      },
      "outputs": [],
      "source": "start_time = time.time()\n\n# Step 1: Advanced preprocessing\nrobust_scaler = RobustScaler()\nX_train_robust = robust_scaler.fit_transform(X_train)\nX_test_robust  = robust_scaler.transform(X_test)\n\n# Step 2: SMOTE if available\nif SMOTE_AVAILABLE:\n    smote = SMOTE(random_state=42)\n    X_train_bal, y_train_bal = smote.fit_resample(X_train_robust, y_train)\nelse:\n    smote = None\n    X_train_bal, y_train_bal = X_train_robust, y_train\n\n# Step 3: Stacking ensemble\nbase_models = [\n    (\"mlp1\", MLPClassifier(\n        hidden_layer_sizes=(128, 64, 32),\n        learning_rate_init=0.001,\n        alpha=0.001,\n        max_iter=1000,\n        early_stopping=True,\n        random_state=42,\n        verbose=False\n    )),\n    (\"mlp2\", MLPClassifier(\n        hidden_layer_sizes=(256, 128, 64),\n        learning_rate=\"adaptive\",\n        learning_rate_init=0.001,\n        alpha=0.01,\n        max_iter=1500,\n        early_stopping=True,\n        validation_fraction=0.15,\n        random_state=43,\n        verbose=False\n    )),\n    (\"mlp3\", MLPClassifier(\n        hidden_layer_sizes=(200, 100, 50),\n        learning_rate_init=0.01,\n        alpha=0.001,\n        max_iter=1000,\n        early_stopping=True,\n        random_state=44,\n        verbose=False\n    )),\n    (\"rf\", RandomForestClassifier(\n        n_estimators=250,\n        max_depth=12,\n        min_samples_split=3,\n        random_state=42\n    )),\n    (\"lr\", LogisticRegression(\n        C=0.05,\n        max_iter=1000,\n        random_state=42\n    )),\n]\n\nmeta_learner = LogisticRegression(C=2.0, max_iter=2000, random_state=42)\n\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=meta_learner,\n    cv=5,\n    stack_method=\"predict_proba\",\n    verbose=0\n)\n\n# Train stacking\nstacking_model.fit(X_train_bal, y_train_bal)\n\n# Step 4: Threshold optimization\ny_proba = stacking_model.predict_proba(X_test_robust)[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_proba)\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\nopt_idx = int(np.argmax(f1_scores))\nopt_threshold = thresholds[opt_idx] if opt_idx < len(thresholds) else 0.5\n\ntrain_time = time.time() - start_time\n\n# Wrap final model\nultimate_model = UltimateOptimizedModel(\n    stacking_model=stacking_model,\n    threshold=opt_threshold,\n    scaler=robust_scaler,\n    smote_model=smote\n)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}